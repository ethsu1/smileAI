{"ast":null,"code":"var _jsxFileName = \"/Users/ethan/Desktop/smileai/src/Camera.js\";\nimport React from 'react';\nimport { Container, Row, Col, Jumbotron } from 'react-bootstrap';\nimport 'bootstrap/dist/css/bootstrap.min.css';\nimport * as blazeface from '@tensorflow-models/blazeface';\nimport '@tensorflow/tfjs-backend-webgl';\nimport * as faceapi from 'face-api.js';\nimport './styles.css';\nimport axios from 'axios';\nlet model, canvas, canvasCtx, video, newImage;\n\nclass Camera extends React.Component {\n  constructor(props) {\n    super(props);\n    this.state = {\n      video: null\n    };\n    this.faceDetect = this.faceDetect.bind(this);\n    this.drawBox = this.drawBox.bind(this);\n  }\n\n  async componentDidMount() {\n    //await faceapi.nets.faceRecognitionNet.loadFromUri('/models');\n    //console.log(faceapi.nets)\n    //model = await blazeface.load();\n    var constraints = {\n      audio: true,\n      video: {\n        width: 720,\n        height: 500\n      },\n      facingMode: {\n        exact: \"user\"\n      }\n    };\n    video = document.querySelector(\"#video\");\n    canvas = document.querySelector(\"#videoCanvas\");\n    video.width = constraints[\"video\"][\"width\"];\n    video.height = constraints[\"video\"][\"height\"];\n    canvas.width = constraints[\"video\"][\"width\"];\n    canvas.height = constraints[\"video\"][\"height\"];\n    canvasCtx = canvas.getContext('2d');\n    canvasCtx.translate(canvas.width, 0);\n    canvasCtx.scale(-1, 1); //var loadModel = this.faceDetect;\n\n    navigator.mediaDevices.getUserMedia(constraints).then(stream => {\n      //var video = document.querySelector(\"#video\");\n      video.srcObject = stream;\n\n      video.onloadedmetadata = () => {\n        video.play();\n      }; //video.onplay = () => loadModel()\n\n    }).catch(function (err) {\n      console.log(err);\n    });\n    newImage = document.createElement('img');\n    document.body.append(newImage);\n  }\n  /*async faceDetect() {\n  \tconst returnTensors = false;\n  \tconst predictions = await model.estimateFaces(video, returnTensors);\n  \t//const regionsToExtract = [ new faceapi.Rect(0, 0, 100, 100)]\n  \t//const predictions = await faceapi.detectAllFaces(video, new faceapi.faceRecognitionNetOptions())\n  \t//const descriptor = await faceapi.computeFaceDescriptor()\n  \t//console.log(descriptor)\n     \tawait this.drawBox(predictions)\n     \trequestAnimationFrame(this.faceDetect)\n  }\n  \tasync drawBox(predictions) {\n  \tif(predictions.length > 0) {\n  \t\tcanvasCtx.drawImage(video, 0,0)\n  \t\tfor (let i = 0; i < predictions.length; i++) {\n  \t\t\tconst start = predictions[i].topLeft;\n  \t\t\tconst end = predictions[i].bottomRight;\n  \t\t\tconst size = [end[0] - start[0], end[1] - start[1]];\n  \t\t\t// Render a bounding box over faces\n  \t\t\tcanvasCtx.beginPath();\n  \t\t\tcanvasCtx.rect(start[0], start[1], size[0], size[1]);\n  \t\t\tcanvasCtx.strokeStyle = 'black';\n  \t\t\tcanvasCtx.stroke(); \n  \t\t\tvar hidden_canvas = document.createElement('canvas');\n  \t\t\thidden_canvas.width = 720\n  \t\t\thidden_canvas.height = 500\n  \t\t\tvar newCtx = hidden_canvas.getContext('2d');\n  \t\t\t//newCtx.drawImage(canvas, canvas.width - size[0]-start[0], start[1], size[0], size[1], 0, 0, size[0],size[1])\n  \t\t\tnewCtx.drawImage(video,0,0,720,500)\n  \t\t\tnewImage.src = hidden_canvas.toDataURL()\n  \t\t\tawait hidden_canvas.toBlob((blob) => {\n  \t\t\t\tconst formData = new FormData();\n  \t\t\t\tformData.append(\"image\", blob);\n  \t\t\t\t//console.log(blob)\n  \t\t\t\taxios({\n  \t\t\t\t\t\tmethod: 'post',\n     \t\t\t\t\turl: 'http://127.0.0.1:8000/',\n     \t\t\t\t\tdata: formData,\n     \t\t\t\t\theaders: {'Content-Type': 'multipart/form-data' }\n  \t\t\t\t})\n  \t\t\t\t.then(res => {\n  \t\t\t\t\tconsole.log(res)\n  \t\t\t\t})\n  \t\t\t\t.catch(err =>{\n  \t\t\t\t\t//console.log(err)\n  \t\t\t\t})\n  \t\t\t})\n      \t}\n      }\n  }*/\n\n\n  render() {\n    return /*#__PURE__*/React.createElement(Container, {\n      fluid: true,\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 106,\n        columnNumber: 4\n      }\n    }, /*#__PURE__*/React.createElement(Jumbotron, {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 107,\n        columnNumber: 6\n      }\n    }, /*#__PURE__*/React.createElement(Row, {\n      className: \"justify-content-md-center\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 108,\n        columnNumber: 7\n      }\n    }, /*#__PURE__*/React.createElement(\"h1\", {\n      className: \"title\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 109,\n        columnNumber: 10\n      }\n    }, \"smileAI\")), /*#__PURE__*/React.createElement(Row, {\n      className: \"justify-content-md-center\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 111,\n        columnNumber: 9\n      }\n    }, /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 112,\n        columnNumber: 9\n      }\n    }, \"Simple AI that detects smiles. Positive vibes only.\")), /*#__PURE__*/React.createElement(Row, {\n      className: \"justify-content-md-center\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 116,\n        columnNumber: 7\n      }\n    }, /*#__PURE__*/React.createElement(\"p\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 117,\n        columnNumber: 8\n      }\n    }, /*#__PURE__*/React.createElement(\"small\", {\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 118,\n        columnNumber: 9\n      }\n    }, \"Neural network built without any deep learning libraries (only numpy).\")))), /*#__PURE__*/React.createElement(Row, {\n      className: \"justify-content-md-center\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 122,\n        columnNumber: 5\n      }\n    }, /*#__PURE__*/React.createElement(Col, {\n      xs: 6,\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 123,\n        columnNumber: 9\n      }\n    }, /*#__PURE__*/React.createElement(\"canvas\", {\n      id: \"videoCanvas\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 124,\n        columnNumber: 10\n      }\n    }), /*#__PURE__*/React.createElement(\"video\", {\n      id: \"video\",\n      autoPlay: true,\n      playsInline: true,\n      src: this.state.video,\n      className: \"video\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 125,\n        columnNumber: 10\n      }\n    }))));\n  }\n\n}\n\nexport default Camera;","map":{"version":3,"sources":["/Users/ethan/Desktop/smileai/src/Camera.js"],"names":["React","Container","Row","Col","Jumbotron","blazeface","faceapi","axios","model","canvas","canvasCtx","video","newImage","Camera","Component","constructor","props","state","faceDetect","bind","drawBox","componentDidMount","constraints","audio","width","height","facingMode","exact","document","querySelector","getContext","translate","scale","navigator","mediaDevices","getUserMedia","then","stream","srcObject","onloadedmetadata","play","catch","err","console","log","createElement","body","append","render"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,SAAQC,SAAR,EAAmBC,GAAnB,EAAwBC,GAAxB,EAA6BC,SAA7B,QAA6C,iBAA7C;AACA,OAAO,sCAAP;AACA,OAAO,KAAKC,SAAZ,MAA2B,8BAA3B;AACA,OAAO,gCAAP;AACA,OAAO,KAAKC,OAAZ,MAAyB,aAAzB;AACA,OAAO,cAAP;AACA,OAAOC,KAAP,MAAkB,OAAlB;AACA,IAAIC,KAAJ,EAAWC,MAAX,EAAkBC,SAAlB,EAA6BC,KAA7B,EAAoCC,QAApC;;AACA,MAAMC,MAAN,SAAqBb,KAAK,CAACc,SAA3B,CAAoC;AACnCC,EAAAA,WAAW,CAACC,KAAD,EAAO;AACjB,UAAMA,KAAN;AACA,SAAKC,KAAL,GAAY;AACXN,MAAAA,KAAK,EAAE;AADI,KAAZ;AAGA,SAAKO,UAAL,GAAkB,KAAKA,UAAL,CAAgBC,IAAhB,CAAqB,IAArB,CAAlB;AACA,SAAKC,OAAL,GAAe,KAAKA,OAAL,CAAaD,IAAb,CAAkB,IAAlB,CAAf;AACA;;AACD,QAAME,iBAAN,GAAyB;AACxB;AACA;AACA;AACG,QAAIC,WAAW,GAAG;AAAEC,MAAAA,KAAK,EAAE,IAAT;AAAeZ,MAAAA,KAAK,EAAE;AAAEa,QAAAA,KAAK,EAAE,GAAT;AAAcC,QAAAA,MAAM,EAAE;AAAtB,OAAtB;AAAkDC,MAAAA,UAAU,EAAE;AAAEC,QAAAA,KAAK,EAAE;AAAT;AAA9D,KAAlB;AACAhB,IAAAA,KAAK,GAAGiB,QAAQ,CAACC,aAAT,CAAuB,QAAvB,CAAR;AACApB,IAAAA,MAAM,GAAGmB,QAAQ,CAACC,aAAT,CAAuB,cAAvB,CAAT;AACHlB,IAAAA,KAAK,CAACa,KAAN,GAAcF,WAAW,CAAC,OAAD,CAAX,CAAqB,OAArB,CAAd;AACEX,IAAAA,KAAK,CAACc,MAAN,GAAeH,WAAW,CAAC,OAAD,CAAX,CAAqB,QAArB,CAAf;AACAb,IAAAA,MAAM,CAACe,KAAP,GAAeF,WAAW,CAAC,OAAD,CAAX,CAAqB,OAArB,CAAf;AACCb,IAAAA,MAAM,CAACgB,MAAP,GAAgBH,WAAW,CAAC,OAAD,CAAX,CAAqB,QAArB,CAAhB;AACAZ,IAAAA,SAAS,GAAGD,MAAM,CAACqB,UAAP,CAAkB,IAAlB,CAAZ;AACApB,IAAAA,SAAS,CAACqB,SAAV,CAAoBtB,MAAM,CAACe,KAA3B,EAAkC,CAAlC;AACHd,IAAAA,SAAS,CAACsB,KAAV,CAAgB,CAAC,CAAjB,EAAoB,CAApB,EAbwB,CActB;;AACCC,IAAAA,SAAS,CAACC,YAAV,CAAuBC,YAAvB,CAAoCb,WAApC,EACCc,IADD,CACOC,MAAD,IAAY;AACd;AACA1B,MAAAA,KAAK,CAAC2B,SAAN,GAAkBD,MAAlB;;AACA1B,MAAAA,KAAK,CAAC4B,gBAAN,GAAyB,MAAM;AAC9B5B,QAAAA,KAAK,CAAC6B,IAAN;AACA,OAFD,CAHc,CAMd;;AACF,KARF,EASEC,KATF,CASQ,UAASC,GAAT,EAAc;AAClBC,MAAAA,OAAO,CAACC,GAAR,CAAYF,GAAZ;AACA,KAXJ;AAYG9B,IAAAA,QAAQ,GAAGgB,QAAQ,CAACiB,aAAT,CAAuB,KAAvB,CAAX;AACAjB,IAAAA,QAAQ,CAACkB,IAAT,CAAcC,MAAd,CAAqBnC,QAArB;AAEN;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAqDAoC,EAAAA,MAAM,GAAE;AACP,wBACC,oBAAC,SAAD;AAAW,MAAA,KAAK,MAAhB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACE,oBAAC,SAAD;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACC,oBAAC,GAAD;AAAK,MAAA,SAAS,EAAC,2BAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACG;AAAI,MAAA,SAAS,EAAC,OAAd;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,iBADH,CADD,eAIG,oBAAC,GAAD;AAAK,MAAA,SAAS,EAAC,2BAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,6DADA,CAJH,eASC,oBAAC,GAAD;AAAK,MAAA,SAAS,EAAC,2BAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,gFADD,CADD,CATD,CADF,eAgBC,oBAAC,GAAD;AAAK,MAAA,SAAS,EAAC,2BAAf;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACI,oBAAC,GAAD;AAAK,MAAA,EAAE,EAAE,CAAT;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACC;AAAQ,MAAA,EAAE,EAAC,aAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MADD,eAEC;AAAO,MAAA,EAAE,EAAC,OAAV;AAAkB,MAAA,QAAQ,EAAE,IAA5B;AAAkC,MAAA,WAAW,EAAE,IAA/C;AAAqD,MAAA,GAAG,EAAE,KAAK/B,KAAL,CAAWN,KAArE;AAA4E,MAAA,SAAS,EAAC,OAAtF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MAFD,CADJ,CAhBD,CADD;AA0BA;;AAzHkC;;AA4HpC,eAAeE,MAAf","sourcesContent":["import React from 'react';\nimport {Container, Row, Col, Jumbotron} from 'react-bootstrap';\nimport 'bootstrap/dist/css/bootstrap.min.css';\nimport * as blazeface from '@tensorflow-models/blazeface';\nimport '@tensorflow/tfjs-backend-webgl';\nimport * as faceapi from 'face-api.js';\nimport './styles.css';\nimport axios from 'axios';\nlet model, canvas,canvasCtx, video, newImage;\nclass Camera extends React.Component{\n\tconstructor(props){\n\t\tsuper(props);\n\t\tthis.state ={\n\t\t\tvideo: null,\n\t\t}\n\t\tthis.faceDetect = this.faceDetect.bind(this)\n\t\tthis.drawBox = this.drawBox.bind(this)\n\t}\n\tasync componentDidMount(){\n\t\t//await faceapi.nets.faceRecognitionNet.loadFromUri('/models');\n\t\t//console.log(faceapi.nets)\n\t\t//model = await blazeface.load();\n    \tvar constraints = { audio: true, video: { width: 720, height: 500}, facingMode: { exact: \"user\" } };\n    \tvideo = document.querySelector(\"#video\");\n\t    canvas = document.querySelector(\"#videoCanvas\");\n\t\tvideo.width = constraints[\"video\"][\"width\"];\n  \t\tvideo.height = constraints[\"video\"][\"height\"];\n  \t\tcanvas.width = constraints[\"video\"][\"width\"];\n\t    canvas.height = constraints[\"video\"][\"height\"];\n\t    canvasCtx = canvas.getContext('2d');\n\t    canvasCtx.translate(canvas.width, 0);\n\t\tcanvasCtx.scale(-1, 1);\n  \t\t//var loadModel = this.faceDetect;\n    \tnavigator.mediaDevices.getUserMedia(constraints)\n    \t.then((stream) => {\n\t        //var video = document.querySelector(\"#video\");\n\t        video.srcObject = stream;\n\t        video.onloadedmetadata = () => {\n\t        \tvideo.play();\n\t        };\n\t        //video.onplay = () => loadModel()\n     \t})\n     \t.catch(function(err) {\n        \tconsole.log(err)\n        }); \n        newImage = document.createElement('img');\n        document.body.append(newImage);\n\n\t}\n\n\t/*async faceDetect() {\n\t\tconst returnTensors = false;\n\t\tconst predictions = await model.estimateFaces(video, returnTensors);\n\t\t//const regionsToExtract = [ new faceapi.Rect(0, 0, 100, 100)]\n\t\t//const predictions = await faceapi.detectAllFaces(video, new faceapi.faceRecognitionNetOptions())\n\t\t//const descriptor = await faceapi.computeFaceDescriptor()\n\t\t//console.log(descriptor)\n    \tawait this.drawBox(predictions)\n    \trequestAnimationFrame(this.faceDetect)\n\t}\n\n\tasync drawBox(predictions) {\n\t\tif(predictions.length > 0) {\n\t\t\tcanvasCtx.drawImage(video, 0,0)\n\t\t\tfor (let i = 0; i < predictions.length; i++) {\n\t\t\t\tconst start = predictions[i].topLeft;\n\t\t\t\tconst end = predictions[i].bottomRight;\n\t\t\t\tconst size = [end[0] - start[0], end[1] - start[1]];\n\t\t\t\t// Render a bounding box over faces\n\t\t\t\tcanvasCtx.beginPath();\n\t\t\t\tcanvasCtx.rect(start[0], start[1], size[0], size[1]);\n\t\t\t\tcanvasCtx.strokeStyle = 'black';\n\t\t\t\tcanvasCtx.stroke(); \n\t\t\t\tvar hidden_canvas = document.createElement('canvas');\n\t\t\t\thidden_canvas.width = 720\n\t\t\t\thidden_canvas.height = 500\n\t\t\t\tvar newCtx = hidden_canvas.getContext('2d');\n\t\t\t\t//newCtx.drawImage(canvas, canvas.width - size[0]-start[0], start[1], size[0], size[1], 0, 0, size[0],size[1])\n\t\t\t\tnewCtx.drawImage(video,0,0,720,500)\n\t\t\t\tnewImage.src = hidden_canvas.toDataURL()\n\t\t\t\tawait hidden_canvas.toBlob((blob) => {\n\t\t\t\t\tconst formData = new FormData();\n\t\t\t\t\tformData.append(\"image\", blob);\n\t\t\t\t\t//console.log(blob)\n\t\t\t\t\taxios({\n\n\t\t\t\t\t\tmethod: 'post',\n    \t\t\t\t\turl: 'http://127.0.0.1:8000/',\n    \t\t\t\t\tdata: formData,\n    \t\t\t\t\theaders: {'Content-Type': 'multipart/form-data' }\n\t\t\t\t\t})\n\t\t\t\t\t.then(res => {\n\t\t\t\t\t\tconsole.log(res)\n\t\t\t\t\t})\n\t\t\t\t\t.catch(err =>{\n\t\t\t\t\t\t//console.log(err)\n\t\t\t\t\t})\n\t\t\t\t})\n\t    \t}\n\t    }\n\t}*/\n\n\n\trender(){\n\t\treturn(\n\t\t\t<Container fluid>\n\t\t\t\t\t<Jumbotron>\n\t\t\t\t\t\t<Row className=\"justify-content-md-center\">\n\t\t\t\t\t  \t\t<h1 className=\"title\">smileAI</h1>\n\t\t\t\t\t  \t</Row>\n\t\t\t\t\t  \t<Row className=\"justify-content-md-center\">\n\t\t\t\t\t\t  <p>\n\t\t\t\t\t\t    Simple AI that detects smiles. Positive vibes only.\n\t\t\t\t\t\t  </p>\n\t\t\t\t\t\t</Row>\n\t\t\t\t\t\t<Row className=\"justify-content-md-center\">\n\t\t\t\t\t\t\t<p>\n\t\t\t\t\t\t\t\t<small>Neural network built without any deep learning libraries (only numpy).</small>\n\t\t\t\t\t\t\t</p>\n\t\t\t\t\t\t</Row>\n\t\t\t\t\t</Jumbotron>\n\t\t\t\t<Row className=\"justify-content-md-center\">\n\t\t\t\t    <Col xs={6}>\n\t\t\t\t    \t<canvas id=\"videoCanvas\"></canvas>\n\t\t\t\t    \t<video id=\"video\" autoPlay={true} playsInline={true} src={this.state.video} className=\"video\"></video>\n\t\t\t\t    </Col>\n\t\t\t\t</Row>\n\t\t\t</Container>\n\n\t\t)\n\t}\n}\n\nexport default Camera;"]},"metadata":{},"sourceType":"module"}