{"ast":null,"code":"var _jsxFileName = \"/Users/ethan/Desktop/smileai/src/Camera.js\";\nimport React from 'react';\nimport 'bootstrap/dist/css/bootstrap.min.css'; //import * as blazeface from '@tensorflow-models/blazeface';\n//import '@tensorflow/tfjs-backend-webgl';\n//import * as faceapi from 'face-api.js';\n\nimport { Button, Grid } from '@material-ui/core';\nimport './styles.css';\nimport axios from 'axios';\nlet model, canvas, canvasCtx, video, newImage;\n\nclass Camera extends React.Component {\n  constructor(props) {\n    super(props);\n    this.state = {\n      video: null\n    }; //this.faceDetect = this.faceDetect.bind(this)\n    //this.drawBox = this.drawBox.bind(this)\n\n    this.sendPhoto = this.sendPhoto.bind(this);\n  }\n\n  async componentDidMount() {\n    //await faceapi.nets.faceRecognitionNet.loadFromUri('/models');\n    //console.log(faceapi.nets)\n    //model = await blazeface.load();\n    var constraints = {\n      audio: true,\n      video: {\n        width: 500,\n        height: 500\n      },\n      facingMode: {\n        exact: \"user\"\n      }\n    };\n    video = document.querySelector(\"#video\");\n    canvas = document.querySelector(\"#canvas\");\n    video.width = constraints[\"video\"][\"width\"];\n    video.height = constraints[\"video\"][\"height\"];\n    canvas.width = constraints[\"video\"][\"width\"];\n    canvas.height = constraints[\"video\"][\"height\"];\n    canvasCtx = canvas.getContext('2d');\n    canvasCtx.translate(canvas.width, 0);\n    canvasCtx.scale(-1, 1); //var loadModel = this.faceDetect;\n\n    navigator.mediaDevices.getUserMedia(constraints).then(stream => {\n      //var video = document.querySelector(\"#video\");\n      video.srcObject = stream;\n\n      video.onloadedmetadata = () => {\n        video.play();\n      }; //video.onplay = () => loadModel()\n\n    }).catch(function (err) {\n      console.log(err);\n    }); //newImage = document.createElement('img');\n    //document.body.append(newImage);\n  }\n\n  sendPhoto() {\n    //newCtx.drawImage(canvas, canvas.width - size[0]-start[0], start[1], size[0], size[1], 0, 0, size[0],size[1])\n    canvasCtx.drawImage(video, 0, 0, 500, 500); //newImage.src = canvas.toDataURL()\n\n    canvas.toBlob(blob => {\n      const formData = new FormData();\n      formData.append(\"image\", blob); //console.log(blob)\n\n      axios({\n        method: 'post',\n        url: 'http://127.0.0.1:8000/',\n        data: formData,\n        headers: {\n          'Content-Type': 'multipart/form-data'\n        }\n      }).then(res => {\n        console.log(res);\n      }).catch(err => {//console.log(err)\n      });\n    });\n  }\n  /*async faceDetect() {\n  \tconst returnTensors = false;\n  \tconst predictions = await model.estimateFaces(video, returnTensors);\n  \t//const regionsToExtract = [ new faceapi.Rect(0, 0, 100, 100)]\n  \t//const predictions = await faceapi.detectAllFaces(video, new faceapi.faceRecognitionNetOptions())\n  \t//const descriptor = await faceapi.computeFaceDescriptor()\n  \t//console.log(descriptor)\n     \tawait this.drawBox(predictions)\n     \trequestAnimationFrame(this.faceDetect)\n  }\n  \tasync drawBox(predictions) {\n  \tif(predictions.length > 0) {\n  \t\tcanvasCtx.drawImage(video, 0,0)\n  \t\tfor (let i = 0; i < predictions.length; i++) {\n  \t\t\tconst start = predictions[i].topLeft;\n  \t\t\tconst end = predictions[i].bottomRight;\n  \t\t\tconst size = [end[0] - start[0], end[1] - start[1]];\n  \t\t\t// Render a bounding box over faces\n  \t\t\tcanvasCtx.beginPath();\n  \t\t\tcanvasCtx.rect(start[0], start[1], size[0], size[1]);\n  \t\t\tcanvasCtx.strokeStyle = 'black';\n  \t\t\tcanvasCtx.stroke(); \n  \t\t\tvar hidden_canvas = document.createElement('canvas');\n  \t\t\thidden_canvas.width = 720\n  \t\t\thidden_canvas.height = 500\n  \t\t\tvar newCtx = hidden_canvas.getContext('2d');\n  \t\t\t//newCtx.drawImage(canvas, canvas.width - size[0]-start[0], start[1], size[0], size[1], 0, 0, size[0],size[1])\n  \t\t\tnewCtx.drawImage(video,0,0,720,500)\n  \t\t\tnewImage.src = hidden_canvas.toDataURL()\n  \t\t\tawait hidden_canvas.toBlob((blob) => {\n  \t\t\t\tconst formData = new FormData();\n  \t\t\t\tformData.append(\"image\", blob);\n  \t\t\t\t//console.log(blob)\n  \t\t\t\taxios({\n  \t\t\t\t\t\tmethod: 'post',\n     \t\t\t\t\turl: 'http://127.0.0.1:8000/',\n     \t\t\t\t\tdata: formData,\n     \t\t\t\t\theaders: {'Content-Type': 'multipart/form-data' }\n  \t\t\t\t})\n  \t\t\t\t.then(res => {\n  \t\t\t\t\tconsole.log(res)\n  \t\t\t\t})\n  \t\t\t\t.catch(err =>{\n  \t\t\t\t\t//console.log(err)\n  \t\t\t\t})\n  \t\t\t})\n      \t}\n      }\n  }*/\n\n\n  render() {\n    return /*#__PURE__*/React.createElement(Grid, {\n      container: true,\n      direction: \"row\",\n      justify: \"center\",\n      alignItems: \"center\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 129,\n        columnNumber: 4\n      }\n    }, /*#__PURE__*/React.createElement(Grid, {\n      container: true,\n      item: true,\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 130,\n        columnNumber: 5\n      }\n    }, /*#__PURE__*/React.createElement(\"video\", {\n      id: \"video\",\n      autoPlay: true,\n      playsInline: true,\n      src: this.state.video,\n      className: \"video\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 131,\n        columnNumber: 6\n      }\n    })), /*#__PURE__*/React.createElement(Grid, {\n      container: true,\n      item: true,\n      xs: 12,\n      spacing: 3,\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 133,\n        columnNumber: 5\n      }\n    }, /*#__PURE__*/React.createElement(\"canvas\", {\n      id: \"canvas\",\n      __self: this,\n      __source: {\n        fileName: _jsxFileName,\n        lineNumber: 134,\n        columnNumber: 6\n      }\n    })));\n  }\n\n}\n\nexport default Camera;","map":{"version":3,"sources":["/Users/ethan/Desktop/smileai/src/Camera.js"],"names":["React","Button","Grid","axios","model","canvas","canvasCtx","video","newImage","Camera","Component","constructor","props","state","sendPhoto","bind","componentDidMount","constraints","audio","width","height","facingMode","exact","document","querySelector","getContext","translate","scale","navigator","mediaDevices","getUserMedia","then","stream","srcObject","onloadedmetadata","play","catch","err","console","log","drawImage","toBlob","blob","formData","FormData","append","method","url","data","headers","res","render"],"mappings":";AAAA,OAAOA,KAAP,MAAkB,OAAlB;AACA,OAAO,sCAAP,C,CACA;AACA;AACA;;AACA,SAASC,MAAT,EAAiBC,IAAjB,QAA6B,mBAA7B;AACA,OAAO,cAAP;AACA,OAAOC,KAAP,MAAkB,OAAlB;AACA,IAAIC,KAAJ,EAAWC,MAAX,EAAkBC,SAAlB,EAA6BC,KAA7B,EAAoCC,QAApC;;AACA,MAAMC,MAAN,SAAqBT,KAAK,CAACU,SAA3B,CAAoC;AACnCC,EAAAA,WAAW,CAACC,KAAD,EAAO;AACjB,UAAMA,KAAN;AACA,SAAKC,KAAL,GAAY;AACXN,MAAAA,KAAK,EAAE;AADI,KAAZ,CAFiB,CAKjB;AACA;;AACA,SAAKO,SAAL,GAAiB,KAAKA,SAAL,CAAeC,IAAf,CAAoB,IAApB,CAAjB;AACA;;AACD,QAAMC,iBAAN,GAAyB;AACxB;AACA;AACA;AACG,QAAIC,WAAW,GAAG;AAAEC,MAAAA,KAAK,EAAE,IAAT;AAAeX,MAAAA,KAAK,EAAE;AAAEY,QAAAA,KAAK,EAAE,GAAT;AAAcC,QAAAA,MAAM,EAAE;AAAtB,OAAtB;AAAkDC,MAAAA,UAAU,EAAE;AAAEC,QAAAA,KAAK,EAAE;AAAT;AAA9D,KAAlB;AACAf,IAAAA,KAAK,GAAGgB,QAAQ,CAACC,aAAT,CAAuB,QAAvB,CAAR;AACAnB,IAAAA,MAAM,GAAGkB,QAAQ,CAACC,aAAT,CAAuB,SAAvB,CAAT;AACHjB,IAAAA,KAAK,CAACY,KAAN,GAAcF,WAAW,CAAC,OAAD,CAAX,CAAqB,OAArB,CAAd;AACEV,IAAAA,KAAK,CAACa,MAAN,GAAeH,WAAW,CAAC,OAAD,CAAX,CAAqB,QAArB,CAAf;AACAZ,IAAAA,MAAM,CAACc,KAAP,GAAeF,WAAW,CAAC,OAAD,CAAX,CAAqB,OAArB,CAAf;AACCZ,IAAAA,MAAM,CAACe,MAAP,GAAgBH,WAAW,CAAC,OAAD,CAAX,CAAqB,QAArB,CAAhB;AACAX,IAAAA,SAAS,GAAGD,MAAM,CAACoB,UAAP,CAAkB,IAAlB,CAAZ;AACAnB,IAAAA,SAAS,CAACoB,SAAV,CAAoBrB,MAAM,CAACc,KAA3B,EAAkC,CAAlC;AACHb,IAAAA,SAAS,CAACqB,KAAV,CAAgB,CAAC,CAAjB,EAAoB,CAApB,EAbwB,CActB;;AACCC,IAAAA,SAAS,CAACC,YAAV,CAAuBC,YAAvB,CAAoCb,WAApC,EACCc,IADD,CACOC,MAAD,IAAY;AACd;AACAzB,MAAAA,KAAK,CAAC0B,SAAN,GAAkBD,MAAlB;;AACAzB,MAAAA,KAAK,CAAC2B,gBAAN,GAAyB,MAAM;AAC9B3B,QAAAA,KAAK,CAAC4B,IAAN;AACA,OAFD,CAHc,CAMd;;AACF,KARF,EASEC,KATF,CASQ,UAASC,GAAT,EAAc;AAClBC,MAAAA,OAAO,CAACC,GAAR,CAAYF,GAAZ;AACA,KAXJ,EAfqB,CA2BlB;AACA;AACN;;AAEDvB,EAAAA,SAAS,GAAE;AACV;AACAR,IAAAA,SAAS,CAACkC,SAAV,CAAoBjC,KAApB,EAA0B,CAA1B,EAA4B,CAA5B,EAA8B,GAA9B,EAAkC,GAAlC,EAFU,CAGV;;AACAF,IAAAA,MAAM,CAACoC,MAAP,CAAeC,IAAD,IAAU;AACxB,YAAMC,QAAQ,GAAG,IAAIC,QAAJ,EAAjB;AACCD,MAAAA,QAAQ,CAACE,MAAT,CAAgB,OAAhB,EAAyBH,IAAzB,EAFuB,CAGvB;;AACAvC,MAAAA,KAAK,CAAC;AACL2C,QAAAA,MAAM,EAAE,MADH;AAEFC,QAAAA,GAAG,EAAE,wBAFH;AAGFC,QAAAA,IAAI,EAAEL,QAHJ;AAIFM,QAAAA,OAAO,EAAE;AAAC,0BAAgB;AAAjB;AAJP,OAAD,CAAL,CAMClB,IAND,CAMMmB,GAAG,IAAI;AACZZ,QAAAA,OAAO,CAACC,GAAR,CAAYW,GAAZ;AACA,OARD,EASCd,KATD,CASOC,GAAG,IAAG,CACZ;AACC,OAXF;AAYC,KAhBF;AAiBA;AAED;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;AAqDAc,EAAAA,MAAM,GAAE;AACP,wBACC,oBAAC,IAAD;AAAM,MAAA,SAAS,MAAf;AAAgB,MAAA,SAAS,EAAC,KAA1B;AAAgC,MAAA,OAAO,EAAC,QAAxC;AAAiD,MAAA,UAAU,EAAC,QAA5D;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACC,oBAAC,IAAD;AAAM,MAAA,SAAS,MAAf;AAAgB,MAAA,IAAI,MAApB;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACC;AAAO,MAAA,EAAE,EAAC,OAAV;AAAkB,MAAA,QAAQ,EAAE,IAA5B;AAAkC,MAAA,WAAW,EAAE,IAA/C;AAAqD,MAAA,GAAG,EAAE,KAAKtC,KAAL,CAAWN,KAArE;AAA4E,MAAA,SAAS,EAAC,OAAtF;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MADD,CADD,eAIC,oBAAC,IAAD;AAAM,MAAA,SAAS,MAAf;AAAgB,MAAA,IAAI,MAApB;AAAqB,MAAA,EAAE,EAAE,EAAzB;AAA6B,MAAA,OAAO,EAAE,CAAtC;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,oBACC;AAAQ,MAAA,EAAE,EAAC,QAAX;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA,MADD,CAJD,CADD;AAYA;;AAlIkC;;AAqIpC,eAAeE,MAAf","sourcesContent":["import React from 'react';\nimport 'bootstrap/dist/css/bootstrap.min.css';\n//import * as blazeface from '@tensorflow-models/blazeface';\n//import '@tensorflow/tfjs-backend-webgl';\n//import * as faceapi from 'face-api.js';\nimport { Button, Grid } from '@material-ui/core';\nimport './styles.css';\nimport axios from 'axios';\nlet model, canvas,canvasCtx, video, newImage;\nclass Camera extends React.Component{\n\tconstructor(props){\n\t\tsuper(props);\n\t\tthis.state ={\n\t\t\tvideo: null,\n\t\t}\n\t\t//this.faceDetect = this.faceDetect.bind(this)\n\t\t//this.drawBox = this.drawBox.bind(this)\n\t\tthis.sendPhoto = this.sendPhoto.bind(this)\n\t}\n\tasync componentDidMount(){\n\t\t//await faceapi.nets.faceRecognitionNet.loadFromUri('/models');\n\t\t//console.log(faceapi.nets)\n\t\t//model = await blazeface.load();\n    \tvar constraints = { audio: true, video: { width: 500, height: 500}, facingMode: { exact: \"user\" } };\n    \tvideo = document.querySelector(\"#video\");\n\t    canvas = document.querySelector(\"#canvas\");\n\t\tvideo.width = constraints[\"video\"][\"width\"];\n  \t\tvideo.height = constraints[\"video\"][\"height\"];\n  \t\tcanvas.width = constraints[\"video\"][\"width\"];\n\t    canvas.height = constraints[\"video\"][\"height\"];\n\t    canvasCtx = canvas.getContext('2d');\n\t    canvasCtx.translate(canvas.width, 0);\n\t\tcanvasCtx.scale(-1, 1);\n  \t\t//var loadModel = this.faceDetect;\n    \tnavigator.mediaDevices.getUserMedia(constraints)\n    \t.then((stream) => {\n\t        //var video = document.querySelector(\"#video\");\n\t        video.srcObject = stream;\n\t        video.onloadedmetadata = () => {\n\t        \tvideo.play();\n\t        };\n\t        //video.onplay = () => loadModel()\n     \t})\n     \t.catch(function(err) {\n        \tconsole.log(err)\n        }); \n        //newImage = document.createElement('img');\n        //document.body.append(newImage);\n\t}\n\n\tsendPhoto(){\t\n\t\t//newCtx.drawImage(canvas, canvas.width - size[0]-start[0], start[1], size[0], size[1], 0, 0, size[0],size[1])\n\t\tcanvasCtx.drawImage(video,0,0,500,500)\n\t\t//newImage.src = canvas.toDataURL()\n\t\tcanvas.toBlob((blob) => {\n\t\tconst formData = new FormData();\n\t\t\tformData.append(\"image\", blob);\n\t\t\t//console.log(blob)\n\t\t\taxios({\n\t\t\t\tmethod: 'post',\n    \t\t\turl: 'http://127.0.0.1:8000/',\n    \t\t\tdata: formData,\n    \t\t\theaders: {'Content-Type': 'multipart/form-data' }\n\t\t\t})\n\t\t\t.then(res => {\n\t\t\t\tconsole.log(res)\n\t\t\t})\n\t\t\t.catch(err =>{\n\t\t\t\t//console.log(err)\n\t\t\t\t})\n\t\t\t})\n\t}\n\n\t/*async faceDetect() {\n\t\tconst returnTensors = false;\n\t\tconst predictions = await model.estimateFaces(video, returnTensors);\n\t\t//const regionsToExtract = [ new faceapi.Rect(0, 0, 100, 100)]\n\t\t//const predictions = await faceapi.detectAllFaces(video, new faceapi.faceRecognitionNetOptions())\n\t\t//const descriptor = await faceapi.computeFaceDescriptor()\n\t\t//console.log(descriptor)\n    \tawait this.drawBox(predictions)\n    \trequestAnimationFrame(this.faceDetect)\n\t}\n\n\tasync drawBox(predictions) {\n\t\tif(predictions.length > 0) {\n\t\t\tcanvasCtx.drawImage(video, 0,0)\n\t\t\tfor (let i = 0; i < predictions.length; i++) {\n\t\t\t\tconst start = predictions[i].topLeft;\n\t\t\t\tconst end = predictions[i].bottomRight;\n\t\t\t\tconst size = [end[0] - start[0], end[1] - start[1]];\n\t\t\t\t// Render a bounding box over faces\n\t\t\t\tcanvasCtx.beginPath();\n\t\t\t\tcanvasCtx.rect(start[0], start[1], size[0], size[1]);\n\t\t\t\tcanvasCtx.strokeStyle = 'black';\n\t\t\t\tcanvasCtx.stroke(); \n\t\t\t\tvar hidden_canvas = document.createElement('canvas');\n\t\t\t\thidden_canvas.width = 720\n\t\t\t\thidden_canvas.height = 500\n\t\t\t\tvar newCtx = hidden_canvas.getContext('2d');\n\t\t\t\t//newCtx.drawImage(canvas, canvas.width - size[0]-start[0], start[1], size[0], size[1], 0, 0, size[0],size[1])\n\t\t\t\tnewCtx.drawImage(video,0,0,720,500)\n\t\t\t\tnewImage.src = hidden_canvas.toDataURL()\n\t\t\t\tawait hidden_canvas.toBlob((blob) => {\n\t\t\t\t\tconst formData = new FormData();\n\t\t\t\t\tformData.append(\"image\", blob);\n\t\t\t\t\t//console.log(blob)\n\t\t\t\t\taxios({\n\n\t\t\t\t\t\tmethod: 'post',\n    \t\t\t\t\turl: 'http://127.0.0.1:8000/',\n    \t\t\t\t\tdata: formData,\n    \t\t\t\t\theaders: {'Content-Type': 'multipart/form-data' }\n\t\t\t\t\t})\n\t\t\t\t\t.then(res => {\n\t\t\t\t\t\tconsole.log(res)\n\t\t\t\t\t})\n\t\t\t\t\t.catch(err =>{\n\t\t\t\t\t\t//console.log(err)\n\t\t\t\t\t})\n\t\t\t\t})\n\t    \t}\n\t    }\n\t}*/\n\n\n\trender(){\n\t\treturn(\n\t\t\t<Grid container direction=\"row\" justify=\"center\" alignItems=\"center\">\n\t\t\t\t<Grid container item>\n\t\t\t\t\t<video id=\"video\" autoPlay={true} playsInline={true} src={this.state.video} className=\"video\"></video>\n\t\t\t\t</Grid>\n\t\t\t\t<Grid container item xs={12} spacing={3}>\n\t\t\t\t\t<canvas id=\"canvas\"></canvas>\n\t\t\t\t</Grid>\n\n\t\t\t</Grid>\n\n\t\t)\n\t}\n}\n\nexport default Camera;"]},"metadata":{},"sourceType":"module"}